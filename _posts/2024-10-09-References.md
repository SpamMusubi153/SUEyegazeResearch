---
layout: page
title: "C.A.S.E. References"
permalink: /references/
---

### References Cited in the CASE Paper

**[1]**	P. Pathirana, S. Senarath, D. Meedeniya, and S. Jayarathna, “Eye gaze estimation: A survey on deep learning-based approaches,” Expert Syst. Appl., vol. 199, p. 116894, Aug. 2022, doi: 10.1016/j.eswa.2022.116894.

**[2]**	Elvesjo John, Skogo Marten, and Gunnar Elvers, “Method and installation for detecting and following an eye and the gaze direction thereof,” 7572008, Jun. 03, 2004

**[3]**	J. Engel et al., “Project Aria: A New Tool for Egocentric Multi-Modal AI Research,” Oct. 01, 2023, arXiv: arXiv:2308.13561. doi: 10.48550/arXiv.2308.13561.

**[4]**	“Field Metrics Test Report Tobii Pro Fusion,” 2, Jun. 2021.

**[5]**	S. Park, S. D. Mello, P. Molchanov, U. Iqbal, O. Hilliges, and J. Kautz, “Few-Shot Adaptive Gaze Estimation,” in 2019 IEEE/CVF International Conference on Computer Vision (ICCV), Seoul, Korea (South): IEEE, Oct. 2019, pp. 9367–9376. doi: 10.1109/ICCV.2019.00946.



### Useful Additional Reading

**[6]**	D. Cazzato, M. Leo, C. Distante, and H. Voos, “When I Look into Your Eyes: A Survey on Computer Vision Contributions for Human Gaze Estimation and Tracking,” Sensors, vol. 20, no. 13, p. 3739, Jul. 2020, doi: 10.3390/s20133739.

**[7]**	Eye Tracking in User Experience Design. Accessed: Sep. 16, 2024. [Online]. Available: https://learning.oreilly.com/library/view/eye-tracking-in/9780124081383/

**[8]**	A. Kar and P. Corcoran, “Performance Evaluation Strategies for Eye Gaze Estimation Systems with Quantitative Metrics and Visualizations,” Sensors, vol. 18, no. 9, p. 3151, Sep. 2018, doi: 10.3390/s18093151.

**[9]**	J. Oh, Y. Lee, J. Yoo, and S. Kwon, “Improved Feature-Based Gaze Estimation Using Self-Attention Module and Synthetic Eye Images,” Sensors, vol. 22, no. 11, p. 4026, May 2022, doi: 10.3390/s22114026.

**[10]**	X. Zhang, Y. Sugano, M. Fritz, and A. Bulling, “Appearance-based gaze estimation in the wild,” in 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Jun. 2015, pp. 4511–4520. doi: 10.1109/CVPR.2015.7299081.

**[11]**	T. Fischer, H. J. Chang, and Y. Demiris, “RT-GENE: Real-Time Eye Gaze Estimation in Natural Environments,” presented at the Proceedings of the European Conference on Computer Vision (ECCV), 2018, pp. 334–352. Accessed: Sep. 17, 2024. [Online]. Available: https://openaccess.thecvf.com/content_ECCV_2018/html/Tobias_Fischer_RT-GENE_Real-Time_Eye_ECCV_2018_paper.html

**[12]**	D. E. Worrall, S. J. Garbin, D. Turmukhambetov, and G. J. Brostow, “Interpretable Transformations with Encoder-Decoder Networks,” in 2017 IEEE International Conference on Computer Vision (ICCV), Venice: IEEE, Oct. 2017, pp. 5737–5746. doi: 10.1109/ICCV.2017.611.

**[13]**	A. Hassani, S. Walton, N. Shah, A. Abuduweili, J. Li, and H. Shi, “Escaping the Big Data Paradigm with Compact Transformers,” Jun. 07, 2022, arXiv: arXiv:2104.05704. Accessed: Sep. 03, 2024. [Online]. Available: http://arxiv.org/abs/2104.05704

**[14]**	Y. Zheng, S. Park, X. Zhang, S. D. Mello, and O. Hilliges, “Self-Learning Transformations for Improving Gaze and Head Redirection”.

**[15]**	A. Ruzzi et al., “GazeNeRF: 3D-Aware Gaze Redirection with Neural Radiance Fields,” in 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Vancouver, BC, Canada: IEEE, Jun. 2023, pp. 9676–9685. doi: 10.1109/CVPR52729.2023.00933.